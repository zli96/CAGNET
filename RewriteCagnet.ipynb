{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RewriteCagnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoOKmKZHCw1PuYxpACRqBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zli96/CAGNET/blob/master/RewriteCagnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3jF7-kRfZlD",
        "outputId": "d4dc63b9-7f82-4afe-80cf-e0b48d8fd71e"
      },
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 10.4 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 407 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.0 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWhIT1D-hB7C"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCNFunc(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inputs, weight, adj_matrix, func):\n",
        "        global run\n",
        "        # inputs: H\n",
        "        # adj_matrix: A\n",
        "        # weight: W\n",
        "        # func: sigma\n",
        "\n",
        "        # adj_matrix = adj_matrix.to_dense()\n",
        "        ctx.save_for_backward(inputs, weight, adj_matrix)\n",
        "        ctx.func = func\n",
        "\n",
        "        z = torch.sparse.mm(adj_matrix, inputs)\n",
        "        z = torch.mm(z, weight)\n",
        "\n",
        "        z.requires_grad = True\n",
        "        ctx.z = z\n",
        "\n",
        "        # if activations:\n",
        "        if func is F.log_softmax:\n",
        "            h = func(z, dim=1)\n",
        "        elif func is F.relu:\n",
        "            h = func(z)\n",
        "        else:\n",
        "            h = z\n",
        "\n",
        "        return h\n",
        "        # else:\n",
        "        #     return z\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        global run\n",
        "\n",
        "        inputs, weight, adj_matrix = ctx.saved_tensors\n",
        "        func = ctx.func\n",
        "        z = ctx.z\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            if func is F.log_softmax:\n",
        "                func_eval = func(z, dim=1)\n",
        "            elif func is F.relu:\n",
        "                func_eval = func(z)\n",
        "            else:\n",
        "                func_eval = z\n",
        "\n",
        "            sigmap = torch.autograd.grad(outputs=func_eval, inputs=z, grad_outputs=grad_output)[0]\n",
        "            grad_output = sigmap\n",
        "\n",
        "        # First backprop \n",
        "        ag = torch.sparse.mm(adj_matrix, grad_output)\n",
        "\n",
        "        grad_input = torch.mm(ag, weight.t())\n",
        "\n",
        "        # Second backprop equation (reuses the A * G^l computation)\n",
        "        grad_weight = torch.mm(inputs.t(), ag)\n",
        "\n",
        "        return grad_input, grad_weight, None, None, None, None, None, None\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDVTqsnmvg_2",
        "outputId": "ac7bc53a-781d-4b39-f35d-732c9f4f9d43"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWEBI8f2og1O"
      },
      "source": [
        "def train(inputs, weight1, weight2, adj_matrix, optimizer, data):\n",
        "    outputs = GCNFunc.apply(inputs, weight1, adj_matrix, F.relu)\n",
        "    outputs = GCNFunc.apply(outputs, weight2, adj_matrix, F.log_softmax)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Note: bool type removes warnings, unsure of perf penalty\n",
        "    loss = F.nll_loss(outputs[data.train_mask.bool()], data.y[data.train_mask.bool()])\n",
        "    print('loss: {:.4f}'.format(loss))\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WEytEIKbYxJ"
      },
      "source": [
        "def test(outputs, data):\n",
        "    logits, accs = outputs, []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "\n",
        "    if len(accs) == 1:\n",
        "        accs.append(0)\n",
        "        accs.append(0)\n",
        "\n",
        "    return accs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wzS17IQEWn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edcb0d2d-3b60-4b4c-fc08-a7b01a0788a7"
      },
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "dataset = 'Cora'\n",
        "dataset = Planetoid('../data', dataset, transform=T.NormalizeFeatures())\n",
        "data = dataset[0]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd5idUrLnMy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b7e824-c325-4e5e-f3ca-dc6a5365943f"
      },
      "source": [
        "from torch.nn import Parameter\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "inputs = data\n",
        "num_features = dataset.num_features\n",
        "mid_layer = 16\n",
        "epochs=100\n",
        "num_classes=dataset.num_classes\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.manual_seed(0)\n",
        "weight1_nonleaf = torch.rand(num_features, mid_layer, requires_grad=True)\n",
        "weight1_nonleaf = weight1_nonleaf.to(device)\n",
        "weight1_nonleaf.retain_grad()\n",
        "\n",
        "weight2_nonleaf = torch.rand(mid_layer, num_classes, requires_grad=True)\n",
        "weight2_nonleaf = weight2_nonleaf.to(device)\n",
        "weight2_nonleaf.retain_grad()\n",
        "\n",
        "weight1 = Parameter(weight1_nonleaf)\n",
        "weight2 = Parameter(weight2_nonleaf)\n",
        "\n",
        "optimizer = torch.optim.Adam([weight1, weight2], lr=0.01)\n",
        "temp_value = np.ones((data.edge_index.shape[1],),dtype='float32')\n",
        "idx_array=data.edge_index.data.numpy()\n",
        "num_nodes = data.x.size()[0]\n",
        "adj_matrix = torch.sparse_coo_tensor((idx_array[0,:],idx_array[1,:]), temp_value, ( num_nodes, num_nodes))\n",
        "inputs.x  = inputs.x.to(device)\n",
        "adj_matrix = adj_matrix.to(device)\n",
        "data = data.to(device)\n",
        "for epoch in range(1, epochs):\n",
        "  outputs = train(inputs.x, weight1, weight2, adj_matrix, optimizer, data)\n",
        "  train_acc, val_acc, test_acc = test(outputs, data)\n",
        "  log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
        "  print(log.format(epoch, train_acc, val_acc, test_acc))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 78.0500\n",
            "Epoch: 001, Train: 0.1429, Val: 0.1140, Test: 0.1030\n",
            "loss: 69.7698\n",
            "Epoch: 002, Train: 0.1857, Val: 0.1240, Test: 0.1190\n",
            "loss: 64.5411\n",
            "Epoch: 003, Train: 0.1857, Val: 0.0900, Test: 0.1060\n",
            "loss: 58.6881\n",
            "Epoch: 004, Train: 0.1929, Val: 0.0960, Test: 0.1140\n",
            "loss: 52.8371\n",
            "Epoch: 005, Train: 0.2000, Val: 0.1320, Test: 0.1260\n",
            "loss: 49.9903\n",
            "Epoch: 006, Train: 0.1500, Val: 0.0620, Test: 0.0720\n",
            "loss: 45.9134\n",
            "Epoch: 007, Train: 0.1500, Val: 0.0620, Test: 0.0720\n",
            "loss: 40.9685\n",
            "Epoch: 008, Train: 0.1643, Val: 0.0740, Test: 0.0820\n",
            "loss: 36.2557\n",
            "Epoch: 009, Train: 0.2286, Val: 0.1040, Test: 0.1100\n",
            "loss: 31.8722\n",
            "Epoch: 010, Train: 0.3000, Val: 0.1680, Test: 0.1680\n",
            "loss: 28.7828\n",
            "Epoch: 011, Train: 0.3071, Val: 0.1880, Test: 0.2150\n",
            "loss: 27.4489\n",
            "Epoch: 012, Train: 0.1714, Val: 0.1280, Test: 0.1440\n",
            "loss: 24.8376\n",
            "Epoch: 013, Train: 0.1857, Val: 0.1300, Test: 0.1480\n",
            "loss: 21.6600\n",
            "Epoch: 014, Train: 0.2643, Val: 0.1660, Test: 0.1870\n",
            "loss: 18.8792\n",
            "Epoch: 015, Train: 0.2643, Val: 0.1800, Test: 0.1970\n",
            "loss: 16.1624\n",
            "Epoch: 016, Train: 0.2929, Val: 0.2040, Test: 0.2260\n",
            "loss: 14.4107\n",
            "Epoch: 017, Train: 0.2357, Val: 0.3420, Test: 0.3520\n",
            "loss: 14.0943\n",
            "Epoch: 018, Train: 0.2286, Val: 0.3300, Test: 0.3440\n",
            "loss: 12.7433\n",
            "Epoch: 019, Train: 0.2714, Val: 0.3700, Test: 0.3670\n",
            "loss: 12.7434\n",
            "Epoch: 020, Train: 0.2500, Val: 0.3140, Test: 0.2920\n",
            "loss: 11.5543\n",
            "Epoch: 021, Train: 0.2286, Val: 0.2920, Test: 0.2670\n",
            "loss: 9.4783\n",
            "Epoch: 022, Train: 0.2786, Val: 0.3060, Test: 0.2830\n",
            "loss: 6.9053\n",
            "Epoch: 023, Train: 0.3286, Val: 0.3460, Test: 0.3390\n",
            "loss: 4.7894\n",
            "Epoch: 024, Train: 0.4929, Val: 0.4280, Test: 0.4400\n",
            "loss: 3.5872\n",
            "Epoch: 025, Train: 0.5500, Val: 0.3980, Test: 0.4180\n",
            "loss: 2.4275\n",
            "Epoch: 026, Train: 0.5357, Val: 0.3840, Test: 0.3940\n",
            "loss: 1.3020\n",
            "Epoch: 027, Train: 0.6571, Val: 0.4540, Test: 0.4670\n",
            "loss: 1.5298\n",
            "Epoch: 028, Train: 0.6214, Val: 0.4760, Test: 0.4610\n",
            "loss: 2.7184\n",
            "Epoch: 029, Train: 0.3857, Val: 0.2980, Test: 0.2810\n",
            "loss: 3.7300\n",
            "Epoch: 030, Train: 0.3071, Val: 0.2360, Test: 0.2380\n",
            "loss: 4.1421\n",
            "Epoch: 031, Train: 0.2857, Val: 0.2320, Test: 0.2300\n",
            "loss: 4.0042\n",
            "Epoch: 032, Train: 0.3286, Val: 0.2500, Test: 0.2360\n",
            "loss: 3.4846\n",
            "Epoch: 033, Train: 0.3857, Val: 0.2860, Test: 0.2640\n",
            "loss: 2.7588\n",
            "Epoch: 034, Train: 0.5214, Val: 0.3860, Test: 0.3540\n",
            "loss: 2.0532\n",
            "Epoch: 035, Train: 0.6500, Val: 0.4800, Test: 0.4620\n",
            "loss: 1.8207\n",
            "Epoch: 036, Train: 0.7571, Val: 0.5580, Test: 0.5600\n",
            "loss: 1.7617\n",
            "Epoch: 037, Train: 0.7857, Val: 0.5700, Test: 0.5990\n",
            "loss: 1.7244\n",
            "Epoch: 038, Train: 0.7714, Val: 0.5440, Test: 0.5820\n",
            "loss: 1.6858\n",
            "Epoch: 039, Train: 0.7429, Val: 0.5320, Test: 0.5720\n",
            "loss: 1.7142\n",
            "Epoch: 040, Train: 0.7214, Val: 0.5120, Test: 0.5440\n",
            "loss: 1.8291\n",
            "Epoch: 041, Train: 0.6714, Val: 0.4680, Test: 0.5100\n",
            "loss: 1.8521\n",
            "Epoch: 042, Train: 0.6786, Val: 0.4720, Test: 0.5110\n",
            "loss: 1.7247\n",
            "Epoch: 043, Train: 0.6929, Val: 0.4760, Test: 0.5090\n",
            "loss: 1.5087\n",
            "Epoch: 044, Train: 0.6929, Val: 0.4860, Test: 0.5280\n",
            "loss: 1.2833\n",
            "Epoch: 045, Train: 0.7571, Val: 0.5360, Test: 0.5630\n",
            "loss: 1.1110\n",
            "Epoch: 046, Train: 0.7714, Val: 0.5560, Test: 0.5860\n",
            "loss: 0.9812\n",
            "Epoch: 047, Train: 0.8071, Val: 0.5680, Test: 0.5950\n",
            "loss: 0.8872\n",
            "Epoch: 048, Train: 0.8429, Val: 0.5960, Test: 0.6170\n",
            "loss: 0.8110\n",
            "Epoch: 049, Train: 0.8714, Val: 0.6240, Test: 0.6350\n",
            "loss: 0.7489\n",
            "Epoch: 050, Train: 0.8714, Val: 0.6500, Test: 0.6700\n",
            "loss: 0.7023\n",
            "Epoch: 051, Train: 0.8857, Val: 0.6820, Test: 0.6930\n",
            "loss: 0.6762\n",
            "Epoch: 052, Train: 0.9000, Val: 0.7080, Test: 0.6990\n",
            "loss: 0.6779\n",
            "Epoch: 053, Train: 0.9214, Val: 0.7100, Test: 0.6910\n",
            "loss: 0.7033\n",
            "Epoch: 054, Train: 0.9071, Val: 0.6920, Test: 0.6970\n",
            "loss: 0.7318\n",
            "Epoch: 055, Train: 0.9071, Val: 0.6920, Test: 0.6900\n",
            "loss: 0.7528\n",
            "Epoch: 056, Train: 0.9071, Val: 0.6840, Test: 0.6760\n",
            "loss: 0.7621\n",
            "Epoch: 057, Train: 0.9071, Val: 0.6880, Test: 0.6730\n",
            "loss: 0.7588\n",
            "Epoch: 058, Train: 0.8929, Val: 0.6840, Test: 0.6660\n",
            "loss: 0.7439\n",
            "Epoch: 059, Train: 0.8929, Val: 0.6880, Test: 0.6610\n",
            "loss: 0.7210\n",
            "Epoch: 060, Train: 0.8929, Val: 0.6800, Test: 0.6730\n",
            "loss: 0.6976\n",
            "Epoch: 061, Train: 0.9000, Val: 0.6940, Test: 0.6830\n",
            "loss: 0.6740\n",
            "Epoch: 062, Train: 0.8714, Val: 0.6980, Test: 0.6880\n",
            "loss: 0.6395\n",
            "Epoch: 063, Train: 0.8714, Val: 0.7000, Test: 0.6970\n",
            "loss: 0.6014\n",
            "Epoch: 064, Train: 0.9071, Val: 0.7020, Test: 0.6970\n",
            "loss: 0.5731\n",
            "Epoch: 065, Train: 0.9143, Val: 0.7020, Test: 0.7040\n",
            "loss: 0.5533\n",
            "Epoch: 066, Train: 0.9214, Val: 0.7100, Test: 0.7090\n",
            "loss: 0.5381\n",
            "Epoch: 067, Train: 0.9214, Val: 0.7120, Test: 0.7130\n",
            "loss: 0.5257\n",
            "Epoch: 068, Train: 0.9214, Val: 0.7060, Test: 0.7140\n",
            "loss: 0.5152\n",
            "Epoch: 069, Train: 0.9143, Val: 0.6960, Test: 0.7110\n",
            "loss: 0.5062\n",
            "Epoch: 070, Train: 0.9143, Val: 0.6980, Test: 0.7090\n",
            "loss: 0.4989\n",
            "Epoch: 071, Train: 0.9071, Val: 0.7000, Test: 0.7120\n",
            "loss: 0.4932\n",
            "Epoch: 072, Train: 0.9286, Val: 0.6960, Test: 0.7080\n",
            "loss: 0.4875\n",
            "Epoch: 073, Train: 0.9357, Val: 0.6960, Test: 0.7060\n",
            "loss: 0.4806\n",
            "Epoch: 074, Train: 0.9286, Val: 0.6980, Test: 0.7030\n",
            "loss: 0.4728\n",
            "Epoch: 075, Train: 0.9429, Val: 0.6920, Test: 0.6980\n",
            "loss: 0.4649\n",
            "Epoch: 076, Train: 0.9429, Val: 0.6880, Test: 0.6910\n",
            "loss: 0.4580\n",
            "Epoch: 077, Train: 0.9429, Val: 0.6920, Test: 0.6870\n",
            "loss: 0.4526\n",
            "Epoch: 078, Train: 0.9500, Val: 0.6840, Test: 0.6860\n",
            "loss: 0.4485\n",
            "Epoch: 079, Train: 0.9500, Val: 0.6720, Test: 0.6720\n",
            "loss: 0.4454\n",
            "Epoch: 080, Train: 0.9571, Val: 0.6680, Test: 0.6670\n",
            "loss: 0.4425\n",
            "Epoch: 081, Train: 0.9571, Val: 0.6660, Test: 0.6680\n",
            "loss: 0.4393\n",
            "Epoch: 082, Train: 0.9571, Val: 0.6660, Test: 0.6690\n",
            "loss: 0.4353\n",
            "Epoch: 083, Train: 0.9643, Val: 0.6660, Test: 0.6700\n",
            "loss: 0.4307\n",
            "Epoch: 084, Train: 0.9643, Val: 0.6660, Test: 0.6690\n",
            "loss: 0.4260\n",
            "Epoch: 085, Train: 0.9643, Val: 0.6680, Test: 0.6690\n",
            "loss: 0.4213\n",
            "Epoch: 086, Train: 0.9643, Val: 0.6680, Test: 0.6700\n",
            "loss: 0.4168\n",
            "Epoch: 087, Train: 0.9643, Val: 0.6680, Test: 0.6710\n",
            "loss: 0.4125\n",
            "Epoch: 088, Train: 0.9643, Val: 0.6760, Test: 0.6660\n",
            "loss: 0.4083\n",
            "Epoch: 089, Train: 0.9643, Val: 0.6780, Test: 0.6660\n",
            "loss: 0.4041\n",
            "Epoch: 090, Train: 0.9643, Val: 0.6820, Test: 0.6700\n",
            "loss: 0.4000\n",
            "Epoch: 091, Train: 0.9643, Val: 0.6840, Test: 0.6730\n",
            "loss: 0.3960\n",
            "Epoch: 092, Train: 0.9643, Val: 0.6880, Test: 0.6750\n",
            "loss: 0.3920\n",
            "Epoch: 093, Train: 0.9643, Val: 0.6900, Test: 0.6810\n",
            "loss: 0.3881\n",
            "Epoch: 094, Train: 0.9714, Val: 0.6940, Test: 0.6850\n",
            "loss: 0.3844\n",
            "Epoch: 095, Train: 0.9786, Val: 0.6960, Test: 0.6880\n",
            "loss: 0.3809\n",
            "Epoch: 096, Train: 0.9786, Val: 0.6960, Test: 0.6880\n",
            "loss: 0.3778\n",
            "Epoch: 097, Train: 0.9786, Val: 0.6960, Test: 0.6880\n",
            "loss: 0.3749\n",
            "Epoch: 098, Train: 0.9857, Val: 0.7000, Test: 0.6890\n",
            "loss: 0.3723\n",
            "Epoch: 099, Train: 0.9857, Val: 0.7020, Test: 0.6910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juIz4HkQoEd3",
        "outputId": "95224ec1-c2fb-464d-c10b-fe1dba6af379"
      },
      "source": [
        "data.x.size()[0]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2708"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}